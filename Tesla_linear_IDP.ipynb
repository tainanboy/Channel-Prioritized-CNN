{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import math\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.set_device(0)\n",
    "# specify dtype\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "print(torch.cuda.device_count())\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "momentum = 0.9\n",
    "num_epoch = 10\n",
    "show_every = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "data_path = '/data/put_data/frank840925/IDP/data'\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_path, \n",
    "                                        train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=data_path, \n",
    "                                       train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(trainset.__len__())\n",
    "print(testset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def plot_losses(loss_history1=None, loss_history2=None):\n",
    "    plt.clf()\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    if loss_history1:\n",
    "        ax1.plot(loss_history1, color=\"blue\", label=\"train\")\n",
    "    if loss_history2:\n",
    "        ax1.plot(loss_history2, color=\"green\", label=\"test\")\n",
    "    #ax2 = ax1.twinx()\n",
    "    #ax2.set_yscale('log')\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"loss\") \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.title(\"Cross-entropy loss\")\n",
    "    plt.savefig('output_losses.png')\n",
    "\n",
    "def plot_accuracy(accuracy1=None, accuracy2=None):\n",
    "    plt.clf()\n",
    "    fig2 = plt.figure()\n",
    "    ax1 = fig2.add_subplot(111)\n",
    "    if accuracy1:\n",
    "        ax1.plot(accuracy1, color=\"red\", label=\"train\")\n",
    "    if accuracy2:\n",
    "        ax1.plot(accuracy2, color=\"black\", label=\"test\")\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.title(\"Train/Test accuracy\") \n",
    "    plt.savefig('accuracy.png')\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case3\n",
      "torch.Size([1, 5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "def set_zeros(c, idp_low, idp_high):\n",
    "    N, C, H, W = c.size()\n",
    "    if int(C*idp_low)!=0 and int(C*idp_high)!=C:\n",
    "        print('case1')\n",
    "        non_zeros = c[:,int(C*idp_low):int(C*idp_high), :,:].clone()\n",
    "        low = Variable(torch.zeros(N, int(C*idp_low), H, W)).type(dtype)\n",
    "        high = Variable(torch.zeros(N, C-int(C*idp_high), H, W)).type(dtype)\n",
    "        c = torch.cat([low, non_zeros, high],1).type(dtype)\n",
    "        return c\n",
    "    elif int(C*idp_high)!=C:\n",
    "        print('case2')\n",
    "        non_zeros = c[:,int(C*idp_low):int(C*idp_high), :,:].clone()\n",
    "        high = Variable(torch.zeros(N, C-int(C*idp_high), H, W)).type(dtype)\n",
    "        c = torch.cat([non_zeros, high],1).type(dtype)\n",
    "        return c\n",
    "    elif int(C*idp_low)!=0:\n",
    "        print('case3')\n",
    "        non_zeros = c[:,int(C*idp_low):int(C*idp_high), :,:].clone()\n",
    "        low = Variable(torch.zeros(N, int(C*idp_low), H, W)).type(dtype)\n",
    "        c = torch.cat([low, non_zeros],1).type(dtype)\n",
    "        return c\n",
    "    else:\n",
    "        return c\n",
    "\n",
    "x = Variable(torch.randn(1,5,3,3)).type(dtype)\n",
    "out = set_zeros(x, 0.5, 1)\n",
    "print(out.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # read in N, C, H, W\n",
    "        N, C, H, W = x.size()\n",
    "        # flatten the C * H * W values into a single vector per image\n",
    "        return x.view(N, -1)  \n",
    "    \n",
    "class idp_tensor(nn.Module):\n",
    "    def __init__(self, idp):\n",
    "        super(idp_tensor, self).__init__()\n",
    "        self.idp = idp\n",
    "    def forward(self, c):\n",
    "        #c is an input tensor, size N*C*H*W\n",
    "        N, C, H, W = c.size()\n",
    "        non_zero_channel = int(C*(self.idp))\n",
    "        if C-non_zero_channel > 0:\n",
    "            zeros = Variable(torch.zeros(N, C-non_zero_channel, H, W)).type(dtype)\n",
    "            c = torch.cat([c[:, :non_zero_channel, :, :].clone(), zeros], 1).type(dtype)\n",
    "            #c[:, non_zero_channel:, :, :] = zeros\n",
    "            return c\n",
    "        else:\n",
    "            #idp is 1\n",
    "            return c\n",
    "\n",
    "class range_idp_tensor(nn.Module):\n",
    "    #idp is 0~1\n",
    "    def __init__(self, idp_low, idp_high):\n",
    "        super(range_idp_tensor, self).__init__()\n",
    "        self.idp_low = idp_low\n",
    "        self.idp_high = idp_high\n",
    "    \n",
    "    def forward(self, c):\n",
    "        #c is an input tensor, size N*C*H*W\n",
    "        #set range C*idp_low:C*idp_high as nonzero\n",
    "        N, C, H, W = c.size()\n",
    "        if int(C*self.idp_low)!=0 and int(C*self.idp_high)!=C:\n",
    "            non_zeros = c[:,int(C*self.idp_low):int(C*self.idp_high), :,:].clone()\n",
    "            low = Variable(torch.zeros(N, int(C*self.idp_low), H, W)).type(dtype)\n",
    "            high = Variable(torch.zeros(N, C-int(C*self.idp_high), H, W)).type(dtype)\n",
    "            c = torch.cat([low, non_zeros, high],1).type(dtype)\n",
    "            return c\n",
    "        elif int(C*self.idp_high)!=C:\n",
    "            non_zeros = c[:,int(C*self.idp_low):int(C*self.idp_high), :,:].clone()\n",
    "            high = Variable(torch.zeros(N, C-int(C*self.idp_high), H, W)).type(dtype)\n",
    "            c = torch.cat([non_zeros, high],1).type(dtype)\n",
    "            return c\n",
    "        elif int(C*self.idp_low)!=0:\n",
    "            non_zeros = c[:,int(C*self.idp_low):int(C*self.idp_high), :,:].clone()\n",
    "            low = Variable(torch.zeros(N, int(C*self.idp_low), H, W)).type(dtype)\n",
    "            c = torch.cat([low, non_zeros],1).type(dtype)\n",
    "            return c\n",
    "        else:\n",
    "            return c\n",
    "        \n",
    "class func_allone(nn.Module):\n",
    "    def forward(self, x):\n",
    "        #x is an input tensor, size N*C*H*W\n",
    "        #for cnn, functions are applied to each filter\n",
    "        N, C, H, W = x.size()\n",
    "        x = torch.mul(x.clone(), Variable(torch.ones(N, C, H, W), requires_grad=False).type(dtype))\n",
    "        return x\n",
    "\n",
    "class func_linear(nn.Module):\n",
    "    def __init__(self, k=1):\n",
    "        super(func_linear, self).__init__()\n",
    "        self.k = k\n",
    "    def channel_coeff(self, N, C, H, W):\n",
    "        #C is channels, return a list with corresponding index: [k, k/2,...]\n",
    "        #returns a tensor with size N*C*H*W\n",
    "        coeff = list(map(lambda a: 1-(a/(C+1)), range(0,C)))\n",
    "        tensor_list = []\n",
    "        for c in coeff:\n",
    "            coeff_tensor = torch.ones(H, W)\n",
    "            coeff_tensor = torch.mul(coeff_tensor, c)\n",
    "            tensor_list.append(coeff_tensor)\n",
    "        ct = torch.stack(tensor_list, 0)\n",
    "        ct = torch.stack(([ct]*N))\n",
    "        return ct\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()\n",
    "        x = torch.mul(x.clone(), Variable(self.channel_coeff(N, C, H, W), requires_grad=False).type(dtype))\n",
    "        return x\n",
    "        \n",
    "class func_harmonic(nn.Module):\n",
    "    #perform element-wise multiplication to channels in x with coefficient k/n, n is channel index\n",
    "    def __init__(self, k=1):\n",
    "        super(func_harmonic, self).__init__()\n",
    "        self.k = k\n",
    "    def channel_coeff(self, N, C, H, W):\n",
    "        #C is channels, return a list with corresponding index: [k, k/2,...]\n",
    "        #returns a tensor with size N*C*H*W\n",
    "        coeff = list(map(lambda a: self.k/a, range(1,C+1)))\n",
    "        tensor_list = []\n",
    "        for c in coeff:\n",
    "            coeff_tensor = torch.ones(H, W)\n",
    "            coeff_tensor = torch.mul(coeff_tensor, c)\n",
    "            tensor_list.append(coeff_tensor)\n",
    "        ct = torch.stack(tensor_list, 0)\n",
    "        ct = torch.stack(([ct]*N))\n",
    "        return ct\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x is an input tensor, size N*C*H*W\n",
    "        #for cnn, functions are applied to each filter\n",
    "        N, C, H, W = x.size()\n",
    "        x = torch.mul(x.clone(), Variable(self.channel_coeff(N, C, H, W), requires_grad=False).type(dtype))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tesla_coef_idp_VGG(nn.Module):\n",
    "    def __init__(self, idp_low, idp_high, idp_layers, model):\n",
    "        #idp is 0~1, idp_layers is a set, specify which layer in features should apply idp\n",
    "        #now idp_layers is 1,3,6,8,11,13,15,18,20,22,25,27,29\n",
    "        super(tesla_coef_idp_VGG, self).__init__()\n",
    "        self.idp_low = idp_low\n",
    "        self.idp_high = idp_high\n",
    "        self.idp_layers = idp_layers\n",
    "        self.model = model.type(dtype)\n",
    "        self.features = nn.Sequential(*(self.new_features_list()))\n",
    "        self.classifier = nn.Sequential(nn.Linear(512, 512),\n",
    "                              nn.ReLU(inplace=True),\n",
    "                              nn.Dropout(0.5),\n",
    "                              nn.Linear(512, 10))\n",
    "    \n",
    "    def new_features_list(self):\n",
    "        new_layers = []\n",
    "        for i, layer in enumerate(list(self.model.features.children())):\n",
    "            if i not in self.idp_layers:\n",
    "                new_layers.append(layer)\n",
    "            else:\n",
    "                new_layers.append(layer)\n",
    "                new_layers.append(func_linear())\n",
    "                new_layers.append(range_idp_tensor(self.idp_low, self.idp_high))\n",
    "        new_layers.append(Flatten())\n",
    "        return new_layers        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "#net = VGG16(idp=1).type(dtype)\n",
    "idp = 1\n",
    "idp_layers = {1,3,6,8,11,13,15,18,20,22,25,27,29}\n",
    "net = tesla_coef_idp_VGG(0, 1,idp_layers, models.vgg16(pretrained=True)).type(dtype)\n",
    "\n",
    "x = Variable(torch.randn(32, 3, 32, 32)).type(dtype)\n",
    "out = net(x)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD([{'params': net.features.parameters()},\n",
    "#                       {'params': net.classifier.parameters(), 'lr': 1e-3}], lr=3e-4, momentum=momentum)\n",
    "optimizer = optim.SGD(net.parameters(), lr=4e-3, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0975, Train Accuracy: 17.072536788227769 percent\n",
      "Epoch [2/10], Loss: 1.2767, Train Accuracy: 50.195937300063981 percent\n",
      "Epoch [3/10], Loss: 0.7714, Train Accuracy: 73.394513755598211 percent\n",
      "Epoch [4/10], Loss: 0.5702, Train Accuracy: 81.02007357645553 percent\n",
      "Epoch [5/10], Loss: 0.4422, Train Accuracy: 85.408669225847731 percent\n",
      "Epoch [6/10], Loss: 0.3626, Train Accuracy: 88.099808061420347 percent\n",
      "Epoch [7/10], Loss: 0.2939, Train Accuracy: 90.319097888675628 percent\n",
      "Epoch [8/10], Loss: 0.2385, Train Accuracy: 92.130518234165066 percent\n",
      "Epoch [9/10], Loss: 0.2095, Train Accuracy: 93.102207293666027 percent\n",
      "Epoch [10/10], Loss: 0.1743, Train Accuracy: 94.241842610364685 percent\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "# start training\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "    running_loss1 = []\n",
    "    running_loss2 = []\n",
    "    running_correct1=[]\n",
    "    running_correct2=[]\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        correct = 0\n",
    "        net.train(True)\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs).type(dtype), Variable(labels).type(torch.cuda.LongTensor)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        # loss\n",
    "        running_loss1.append(loss.data[0])\n",
    "        # accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        correct = correct/labels.size(0)*100\n",
    "        running_correct1.append(correct)\n",
    "        \n",
    "    for i, tdata in enumerate(testloader, 0):\n",
    "        test_correct = 0\n",
    "        net.train(False)\n",
    "        tinputs, tlabels = tdata\n",
    "        tinputs, tlabels = Variable(tinputs).type(dtype), Variable(tlabels).type(torch.cuda.LongTensor)\n",
    "        toutput = net(tinputs)\n",
    "        tloss = criterion(toutput, tlabels)\n",
    "        running_loss2.append(tloss.data[0])\n",
    "        _, tpredicted = torch.max(toutput.data, 1)\n",
    "        test_correct += (tpredicted == tlabels.data).sum()\n",
    "        test_correct = test_correct/tlabels.size(0)*100\n",
    "        running_correct2.append(test_correct)\n",
    "    \n",
    "    train_loss.append(np.mean(running_loss1))\n",
    "    test_loss.append(np.mean(running_loss2))\n",
    "    train_acc.append(np.mean(running_correct1))\n",
    "    test_acc.append(np.mean(running_correct2))\n",
    "    \n",
    "    #statistics\n",
    "    print('Epoch [%d/%d], Loss: %.4f, Train Accuracy: %r percent' \n",
    "                %(epoch+1, num_epoch, train_loss[-1], train_acc[-1]))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'tesla_linear_idp_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_losses(train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "net.load_state_dict(torch.load('tesla_linear_idp_model.pkl'))\n",
    "# Set net in inference mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_acccuracy(testloader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(Variable(images, volatile=True).type(dtype))\n",
    "        labels = Variable(labels, volatile=True).type(torch.cuda.LongTensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "    acc = 100*correct/total\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (acc))\n",
    "    return acc\n",
    "\n",
    "test_acccuracy(testloader, net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
